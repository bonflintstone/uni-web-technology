<!DOCTYPE html>

<html lang="en">
  <head>
    <title>Lab 1</title>
    <link rel="stylesheet" type="text/css" href="screen.css">
    <script type="text/javascript" src="scriptFunctions.js"></script>
    <meta charset="urf-8"/>
  </head>
  <body class="wrapper change">
    <nav>
      <span class="title">CS 93</span>
      <a href='lab1.html'>Lab 1</a>
      <a href='lab2.html'>Lab 2</a>
      <a href='lab3.html'>Lab 3</a>
      <a href='lab4.html' class='current_page'>Lab 4</a>
    </nav>
    <h1> Questions </h1>
    <section id= question1>
      <h2>Question 1a</h2>
      <p>
        This tokenizer on first sight does a decent job, but diving deeper in the
        output and actually looking at the code reveals that this is one of the most
        basic ways of tokenizing: The string gets lowercased and then split on
        the regular expression '\W', which stands for all non word characters
        (basically everything except letters and digits).
        <br>
        This gives us loads of room for improvement:
      </p>
      <ol>
        <li>  
          Abbreviations with apostrophes can get confused, the "s" in "it's" is the
          same as in "Jesus's". To avoid this we would need some basic language parsing.
        </li>
        <li>
          We do not differentiate between capitalized words and non capitalized words.
          This means names which are also words get confused. Eg. "mr Beans" and
          "cooked beans" are not really differentiated. Of course, to implement it
          differently we would need to also look at the beginning of sentences and then
          it would be hard to decide if the token should be capitalized or not.
        </li>
        <li>
          This is a low level parsing, so words which belong together get tokenized
          as different tokens instead of one. So for example "Free university Amsterdam"
          is three tokens, instead of one which might be semantically more correct.
        </li>
        <li>
          One could also implement some word-rooting to recognize different forms 
          and declinations of words, as well as aliases. But this is strictly
          speaking not part part of tokenizing
        </li>
      </ol>
    </section>
    <section id="question2">
      <h2>Question 1b</h2>
      <p>
        It would make sense to include all of the words in the stop words list as
        they all have relatively low discriminatory power and thereby, disregarding 
        these terms, we can focus our search engine(information retrieval) on important
        keywords that would more closely bring up pages that are of interest. 
      </p>
    </section>
    <section id="question3">
      <h2>Question 1c</h2>
      <p>
        We think that the frequency follows Zipf’s law, which basically means that
        the higher the frequency of the terms is, the lower the rank of the word is.
        Moreover, following Zipf’s law also implies that the most frequent word will
        appear about twice as often as the second most frequent word, and three
        times as often as the third most frequent word, and so on. 
      </p>
    </section>
    <section id="question4">
      <h2>Question 1d</h2>
      <p>
        We also see that Heaps law applies here, which implies that by scanning
        the book we will hit upon the most common words rather quickly like “the
        for example, but will increasingly slower continue to find more infrequent
        new words.
      </p>
    </section>
    <section id="question5">
      <h2>Question 2a</h2>
      <p>
        One can assess the quality of a TF-based search by counting the number of
        relevant pages and non-relevant pages returned. This ties into precision
        which is the amount of relevant pages returned and recall is the fraction
        of relevant pages in the collection that were returned  by the system. Therefore
        the higher the quality of a TF-based search the higher the precision and recall is.
      </p>
    </section>
    <section id='question6'>
      <p>
        The only change here is the value associated with the term and chapter, instead
        of just showing the number of occurrences we now have the new calculated
        values, which are in general lower and seem to be logarithmic proportional
        to the term frequency
        <br>
        The three highest ranking words are
      </p>
      <ol>
        <li>Chapter 5:  pigeon: 6.58</li>
        <li>Chapter 6:  footman 6.75</li>
        <li>Chapter 10: dance 6.77</li>
      </ol>
      <p>
        I don't think one could say they 'describe' the chapter very well, but in
        terms of importance per article they are way better than the TF scores.
      </p>
      <h2>Question 2b</h2>
    </section>
    <section id='question7'>
      <h2>Question 2c</h2>
      <p>
        TFIDF-based search: Pigeon, footman, dance, dormouse, witness (tfidf)
        <br>
        TF-based search: The, she, to, it, and (TF)
        <br>
        In this case, it is obvious to see that the precision for a TFIDF-based search is
        much higher compared to the TF-based search as the five highest ranking
        words have very low discriminatory power and fit on the stop word list. 
      </p>
    </section>
    <section id='question8'>
      <h2>Question 2d</h2>
      <p>
        I will go through each of my suggestions in the same order in which I made
        them:
      </p>
      <ol>
        <li>
          Precision would go up (confusion between it's and Jesus's would go away) while
          recall would stay the same.
        </li>
        <li>Same as previous.</li>
        <li>Same as previous.</li>
        <li>Precision would stay the same but recall would go up</li>
      </p>
    </section>
  </body>
</html>
